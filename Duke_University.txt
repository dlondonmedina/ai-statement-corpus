Duke University official policy on generative AI Artificial Intelligence
https://learninginnovation.duke.edu/ai-and-teaching-at-duke-2/artificial-intelligence-policies-in-syllabi-guidelines-and-considerations/
Updated: January 24, 2024
Wondering if there is a specific Duke policy regarding AI generated content? You have the discretion to define how, if, and when generative AI may be used in your courses. However, the Duke Community Standard has been updated to include the unauthorized use of generative AI as a form of cheating. Plus, the Office of Undergraduate Education offers a syllabus language menu that identifies considerations for writing an AI policy. (NetID login required).

All instructors should update their syllabi to include guidance on the use of generative AI in their class. We encourage faculty to thoughtfully consider their stance on AI use. In this shift in higher education and the rapidly changing AI market, standardized, one-size-fits-all, AI policies are not sustainable in the long term. They may also not account for the varying stances instructors will take regarding AI use in their classrooms. Furthermore, because generative AI is becoming more ubiquitous, you’ll need to consider your personal stance on AI in your own work, as well as in your classrooms. Establishing an AI policy for your class allows you to have meaningful discussions with students on this topic. Being specific about how AI is or isn’t allowed makes the rules clear for students and faculty if there are academic integrity violations as well. 
The following guidelines for developing an AI policy for your syllabus include examples of generative AI policies from instructors, universities, and centers of teaching and learning. For the most part, they have been curated from a crowd-sourced document that we encourage you to explore on your own to find AI policies developed by instructors in your field and represent your level of comfort with the use of AI. Other sources for sample policies are cited in the text itself.
Kevin Gannon (Queens College in Charlotte) argues in an article titled “Should I Write an AI Policy” (NetID login required) that instructors should start with research into the topic before writing their policy. He offers a list for instructors of varied sources that provide a primer on the mechanics of generative AI, pros and cons of AI use, and a grounding in how generative AI can be incorporated into teaching. 
We encourage you to try at least one generative AI tool. A good way to start exploring is to enter the prompts that you give students for an assignment into the AI tool, and see what the tool returns. It is important to learn how to write effective prompts and also explore existing prompt ideas.  Keep an open and curious mindset when considering whether such tools could be helpful to some or all of your students. 
As you define your individualized AI syllabus statement, your reasoning should be grounded in the intellectual work of the course, your discipline and your understanding of critical thought. Some of the questions you might ask yourself are: What will students lose (or gain) by using generative AI in your course? What do you want students to understand about AI and their intellectual development? We encourage you to explain the rationale for your policies with your students and open a conversation about AI with them. Below are three policy examples that detail the instructors’ thinking behind their AI policy.
In a policy example from Joel Gladd (University of Western Idaho) he mentions two guiding principles in a course that will allow the use of AI: “1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. 2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.”
Lis Horowitz (Salem State University) shares a practical reason behind a zero tolerance policy for generative AI in their writing course. “Since writing, analytical, and critical thinking skills are part of the learning outcomes of this course, all writing assignments should be prepared by the student. Developing strong competencies in this area will prepare you for a competitive workplace. Therefore, AI-generated submissions are not permitted and will be treated as plagiarism.”
By defining the idea of integrity, Megan McNamara (UC Santa Cruz) points out what is at stake when we talk about academic honesty and personal growth. As she states, “Integrity – other people’s perception of your word as true – is one of the most valuable assets you can cultivate in life. Being attentive to integrity in academic settings allows others to trust that you have completed work for which you are taking credit.” Her course allows students to use AI in limited ways and this rationale sets the foundation for this use.
Although students may already use generative AI, it does not mean that they understand the limitations of the tool or how to use it properly. 
For example, generative AI is a brand-new source for information and the rules for citation and the general use of AI are still under formation. If you explain to students how to cite content generated from AI sources it reduces the cognitive load of students unsure of how to act within the Duke Community Standard and raise their awareness of what is at stake when interacting to AI content. Most academic style guides have already formulated citation practices for generative AI. Monash University has curated a list of the various AI citation formats.
You have the option to personalize your approach to citations. Some educators are instructing students to submit a transcript of the conversation with generative AI as an appendix to their written work. Another alternative might be a reflective piece as a companion to an assignment, as Pam Harland (Plymouth State University) has done by providing guiding questions for students: “What was your prompt?” “Did you revise the AI model’s original output for your submission?” “Did you ask follow-up questions?” “What did you learn?”
Another aspect of AI literacy is understanding the ethical use of AI and its limitations. When students have information about the hallucinations, bias, and inaccuracies of generative AI, it underscores why AI is not a shortcut to good results. Ethan Mollick (Harvard’s Wharton School) frames it this way, “[d]on’t trust anything it says. If it gives you a number or fact, assume it is wrong unless you either know the answer or can check in with another source. You will be responsible for any errors or omissions provided by the tool. It works best for topics you understand.” The library at University of Northwestern St. Paul has a guide for students with an overview of AI’s shortcomings, plus information on how to verify sources and double check AI responses. 
If you decide to allow generative AI as a tool for learning in your courses, clarify for your students the circumstances in which AI is allowable. Kim Sydow Cambell (University of Texas) offers an example policy that expands upon the tasks that students can ask AI to perform:
“Because the effective use of Artificial Intelligence (AI) tools is increasingly important to the work of technical communicators, their use is sometimes required or allowed in course assignments. AI tools can support a content creator during all phases of their work:
From Howard University of Law, Howard Bruckner explains to students what tasks are acceptable and underscores the students’ responsibilities for ethical use. “Generative AI tools can be invaluable for generating ideas, identifying sources, synthesizing text, and starting to understand what is essential about a topic. But YOU must guide, verify and craft your work product; do not just cut and paste without understanding.”
You may discover that if you allow students to engage with generative AI, specific guidelines may need to accompany each assignment.
Below are policy examples from the University of Delaware’s Center for Teaching and Assessment of Learning, which distill the four basic approaches that instructors can take in their syllabi. We invite you to consider them as starting points in your exploration of what your AI policy will be.
Use prohibited
Students are not allowed to use advanced automated tools (artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2) on assignments in this course. Each student is expected to complete each assignment without substantive assistance from others, including automated tools.
Use only with prior permission
Students are allowed to use advanced automated tools (artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2) on assignments in this course if instructor permission is obtained in advance. Unless given permission to use those tools, each student is expected to complete each assignment without substantive assistance from others, including automated tools.
Use only with acknowledgement
Students are allowed to use advanced automated tools (artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2) on assignments in this course if that use is properly documented and credited. For example, text generated using ChatGPT-3 should include a citation such as: “Chat-GPT-3. (YYYY, Month DD of query). “Text of your query.” Generated using OpenAI. https://chat.openai.com/” Material generated using other tools should follow a similar citation convention.
Use is freely permitted with no acknowledgement
Students are allowed to use advanced automated tools (artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2) on assignments in this course; no special documentation or citation is required.
We don’t recommend AI detection software as a part of your AI policy for three main reasons. 
1. The products are unreliable. Research on AI detection software from MIT highlights the false positive and negative rates. OpenAI (the company behind ChatGPT) withdrew its own detection software due to the software’s unreliability. 
2. Detection software is biased against certain segments of learners, for example non-native speakers, as research from Stanford shows. 
3. As AI changes, detection software cannot keep up. 
If you decide to use detection software, share that information with your students ahead of time. Results from the software should not be the only measure of whether students have cheated. Students can be encouraged to use detection software prior to handing in work to check for originality (although they should be warned of the limitations as well).
Another option is to craft a policy that warns students they will need to speak with you and defend their work if plagiarism is suspected. As Liza Long (College of Western Idaho) explains “[i]f I suspect that you have used ChatGPT, and you have not included the required citation and reflection, then you will need to meet with me either in person or through Zoom to talk about the assignment. This conversation will include knowledge checks for course content.”
If you’d like to discuss your AI policy in more depth, please reach out to learninginnovation@duke.edu. You can also explore our central resource on AI in education.
Sign up for our monthly email newsletter
Contact a Learning Innovation & Lifetime Education consultant or drop into our office hours.
