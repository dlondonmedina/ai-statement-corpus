University of Redlands official policy on generative AI Artificial Intelligence
https://library.redlands.edu/c.php?g=716986&p=10117765
Artificial Intelligence is a broad range of technologies that rely on intelligence and decision-making from computers or algorithms, rather than humans. This can include tools as varied as search engines and speech recognition software. A recent development in this field is generative AI, tools that use algorithms to generate content, such as text, images, and audio, based on user input. The use of generative AI has raised a host of concerns in many areas, including higher education.
Likely the most popular and accessible generative AI tool is ChatGPT. ChatGPT is a generative AI tool that functions similarly to a chatbot and generates text based on user input in a conversational format. 
Because of its popularity and accessibility, ChatGPT has become a popular tool for writers and researchers. While there are some beneficial applications of its use, there are some major drawbacks that need to be considered before using it.
If you're stuck trying to come up with ideas for a topic to research, ChatGPT may help by generating potential topics based on subjects that you input. While you will likely need to clear any final research paper topic with your professor, the generated ideas can help you get started. Examples:
Part of the research process is determining which search terms will generate results that are relevant to your research topic. Tools like ChatGPT can help by generating synonyms or related terms to your topic. The best search terms would be found by examining relevant sources within databases to see how current scholars are describing these topics, but the results generated by ChatGPT can be a good start if your current search terms aren't working. Examples:
If you're having trouble figuring out what the proper structure for an assignment should be, try asking ChatGPT. Since ChatGPT is built on large volumes of text, it's very good at reproducing common structures that show up often in its dataset. While your professors may have specific expectations for their assignments, the structures generated by ChatGPT can help you get started. Examples:
Currently, the University of Redlands does not have a single policy concerning the use of generative AI, like ChatGPT, in coursework. Instead, individual instructors will set the policies for their classes. These policies can range from banning all use of generative AI in any context to freely allowing its use. Always consult with your instructor and your course syllabus when considering use of generative AI in your coursework as the expectations may differ from course to course.
While ChatGPT can generate convincing looking text for an essay, it will likely include inaccuracies that would likely not be present in human written text. For example, ChatGPT tends to write with an authoritative tone, but the information it presents may not always be factual and require cross referencing to confirm their accuracy. Additionally, ChatGPT tends to make references to outside sources that do not actually exist, so you should not rely on ChatGPT generated text for assignments requiring outside research. If you're having trouble with writing a research paper, try working with a writing tutor. In the following examples, try to confirm some of the information, such as outside sources or film plot, that was generated. Examples:
It's important to remember that ChatGPT and other similar generative AI tools are built to model text, not information. As a result, you should not rely on it to "find" sources for your research. Because it's built to generate text, ChatGPT has a tendency to create sources from scratch that resemble real sources but don't actually exist. Sometimes these sources are presented as fictional, but sometimes they are not. If you're presented with information about a research source from ChatGPT, check other sources, such as Google Scholar or the library's Look Up an Item feature, to see if it exists elsewhere. Try this with the sources from the following examples. If you're having trouble with research, contact a librarian for help. Examples:
When using artificial intelligence, be sure to cite it as you would any other source. Here's a list of sources to help you decide how to cite AI.
Because of ChatGPT and other generative AI's tendencies towards presenting information with an authoritative tone, it is important to fact-check information for accuracy. The following resources may be helpful with this.
ChatGPT collects data about its users as they use its system. Currently, OpenAI has not clearly disclosed how they intend to use some of this data, which includes all user-submitted text, that they collect. Additionally, OpenAI and ChatGPT have faced scrutiny in the European Union, which has much stricter privacy laws than the US, leading to a temporary ban in Italy. Users should remain vigilant about the type of information that they input into ChatGPT.
Currently, ChatGPT is free to use as part of a free research preview. In the future, ChatGPT will become a paid product, as is the case with ChatGPT powered by GPT-4 which purports to address some issues with the currently freely available model. OpenAI has not yet disclosed how much access might cost or whether libraries can license access for their patrons. This raises concerns about equity with regard to how much use of ChatGPT and other generative AI tools will be expected in the future versus who will actually have regular access to use them.
Generative AI tools, like ChatGPT, are built using large bodies of data, such as text or images, to pretrain its algorithm. The algorithm then generates new content based on the training data. Any biases already present within the training data can then be reproduced in the newly generated content. Without prior knowledge of these biases, users risk recreating and spreading them further, or worse, not challenging them at all.
There are some ethical concerns raised by using generative AI tools like AI. For example, Time discovered that OpenAI employed teams of workers in Kenya paid less than $2 an hour to label training data for harmful content which required manual review of text containing graphic descriptions of violence and various forms of abuse. Workers described this process as "torture." Additionally, training and running these large language models consumes massive amounts of energy and generates tons in carbon emissions. These issues raise concerns about whether the benefits of these tools are worth the human and environmental costs, particularly in a time where the tech companies behind them are racing to develop bigger and "better" models.
University of Redlands
Tel: (909) 793-2121
Fax: (909) 793-2029
